import os
import cv2
import dlib
import torch
import numpy as np
import torchaudio
from torchvision import models, transforms
import ollama
import gradio as gr
from TTS.api import TTS
from rvc_infer import RVC  # éœ€è¦ RVC æ¨ç†

# -------------------------------
# 1ï¸âƒ£ é¢éƒ¨æ£€æµ‹ & æƒ…ç»ªè¯†åˆ«
# -------------------------------
PREDICTOR_PATH = os.path.join(os.path.dirname(__file__), "shape_predictor_68_face_landmarks.dat")
if not os.path.exists(PREDICTOR_PATH):
    raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ° {PREDICTOR_PATH}ï¼Œè¯·ç¡®ä¿æ–‡ä»¶å·²ä¸‹è½½ï¼")

detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor(PREDICTOR_PATH)

emotion_model = models.resnet18(pretrained=True)
emotion_model.fc = torch.nn.Linear(512, 7)
emotion_model.eval()

emotion_labels = ["angry", "disgust", "fear", "happy", "neutral", "sad", "surprise"]

def detect_emotion(image_path):
    """ä½¿ç”¨ PyTorch è¿›è¡Œæƒ…ç»ªè¯†åˆ«"""
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image = cv2.resize(image, (224, 224))

    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5])
    ])
    image = transform(image).unsqueeze(0)

    with torch.no_grad():
        outputs = emotion_model(image)
        _, predicted = torch.max(outputs, 1)

    return emotion_labels[predicted.item()]

# -------------------------------
# 2ï¸âƒ£ é¢éƒ¨ç‰¹å¾æå–
# -------------------------------
def extract_facial_features(image_path):
    """ä½¿ç”¨ OpenCV å’Œ dlib æå–é¢éƒ¨ç‰¹å¾"""
    img = cv2.imread(image_path)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    faces = detector(gray)

    features = {"face_shape": "unknown", "skin_color": "unknown"}

    for face in faces:
        landmarks = predictor(gray, face)
        features["face_shape"] = "oval" if landmarks.part(0).x < landmarks.part(16).x else "round"
        features["skin_color"] = "light" if np.mean(gray) > 127 else "dark"
    
    return features

# -------------------------------
# 3ï¸âƒ£ ç”Ÿæˆæ­Œè¯ï¼ˆOllama / Gemma:2bï¼‰
# -------------------------------
def generate_lyrics(facial_features, emotion):
    """ç»“åˆé¢éƒ¨ç‰¹å¾å’Œæƒ…ç»ªç”Ÿæˆæ­Œè¯"""
    prompt = f"A poetic song about a person with {facial_features['face_shape']} face and {facial_features['skin_color']} skin, feeling {emotion}."
    
    response = ollama.chat(model="gemma:2b", messages=[{"role": "user", "content": prompt}])
    
    lyrics = response['message']['content']
    if len(lyrics.split()) < 15:
        lyrics += " This song is full of emotions and melodies that flow smoothly."
    
    return lyrics

# -------------------------------
# 4ï¸âƒ£ ç”Ÿæˆæ—‹å¾‹ï¼ˆPyTorch ç‰ˆéŸ³ä¹ç”Ÿæˆï¼‰
# -------------------------------
def generate_melody(emotion):
    """ç¡®ä¿éŸ³é¢‘è‡³å°‘ 2 ç§’"""
    sample_rate = 22050  # ç¡®ä¿é‡‡æ ·ç‡å¤Ÿé«˜
    melody_length = 2  # è‡³å°‘ 2 ç§’
    
    freqs = {
        "happy": 440,
        "sad": 220,
        "angry": 330,
        "neutral": 262,
        "surprise": 523,
    }
    
    frequency = freqs.get(emotion, 262)
    time = torch.linspace(0, melody_length, steps=int(melody_length * sample_rate))  
    melody_wave = 0.5 * torch.sin(2 * np.pi * frequency * time)

    melody_path = "melody.wav"
    torchaudio.save(melody_path, melody_wave.unsqueeze(0), sample_rate)
    
    return melody_path

# -------------------------------
# 5ï¸âƒ£ ä½¿ç”¨ FastPitch è¿›è¡Œ AI æœ—è¯»ï¼ˆå¸¦æ—‹å¾‹ï¼‰
# -------------------------------
def synthesize_fastpitch(lyrics):
    """ä½¿ç”¨ FastPitch è¿›è¡Œè¯­éŸ³åˆæˆï¼ˆå¸¦æ—‹å¾‹çš„ TTSï¼‰"""
    
    tts = TTS("tts_models/en/ljspeech/fast_pitch")  
    output_wav = "fastpitch_output.wav"
    
    # ç”Ÿæˆ TTS è¯­éŸ³
    tts.tts_to_file(text=lyrics, file_path=output_wav, speed=1.2, max_decoder_steps=500)

    return output_wav

# -------------------------------
# 6ï¸âƒ£ ä½¿ç”¨ RVC è¿›è¡Œæ­Œå”±è½¬æ¢
# -------------------------------
def convert_to_singing(input_wav, output_wav="singing_output.wav"):
    """ä½¿ç”¨ RVC å°† TTS æœ—è¯»è½¬æ¢ä¸º AI æ­Œå£°"""
    
    rvc = RVC(model_path="rvc_model.pth")  # éœ€è¦äº‹å…ˆä¸‹è½½ RVC è®­ç»ƒå¥½çš„æ¨¡å‹
    rvc.convert(input_wav, output_wav)

    return output_wav

# -------------------------------
# 7ï¸âƒ£ Gradio ç•Œé¢ï¼ˆåœ¨çº¿æ’­æ”¾ï¼‰
# -------------------------------
def process_image(image):
    """å®Œæ•´çš„ AI éŸ³ä¹ç”Ÿæˆæµç¨‹"""
    cv2.imwrite("input.jpg", image)
      
    # æ£€æµ‹æƒ…ç»ª
    emotion = detect_emotion("input.jpg")
    print(f"ğŸ§ è¯†åˆ«çš„æƒ…ç»ªï¼š{emotion}")  

    # æå–é¢éƒ¨ç‰¹å¾
    features = extract_facial_features("input.jpg")
    
    # ç”Ÿæˆæ­Œè¯ï¼ˆç»“åˆé¢éƒ¨ç‰¹å¾ & æƒ…ç»ªï¼‰
    lyrics = generate_lyrics(features, emotion)
    
    # ç”Ÿæˆæ—‹å¾‹ï¼ˆåŸºäºæƒ…ç»ªï¼‰
    melody = generate_melody(emotion)
    
    # FastPitch ç”Ÿæˆ TTS è¯­éŸ³ï¼ˆå¸¦æ—‹å¾‹ï¼‰
    fastpitch_audio = synthesize_fastpitch(lyrics)
    
    # RVC è½¬æ¢æˆ AI æ­Œå”±
    singing_audio = convert_to_singing(fastpitch_audio)

    return lyrics, melody, singing_audio


interface = gr.Interface(
    fn=process_image,
    inputs=gr.Image(type="numpy"),
    outputs=[
        "text",  
        gr.Audio(type="filepath", format="wav"),  
        gr.Audio(type="filepath", format="wav")   
    ],
    title="AI æ­Œæ›²ç”Ÿæˆå™¨",
    description="ä¸Šä¼ ä¸€å¼ ç…§ç‰‡ï¼ŒAI å°†æ ¹æ®ä½ çš„é¢éƒ¨ç‰¹å¾ç”Ÿæˆä¸€é¦–æ­Œæ›² ğŸµ"
)

if __name__ == "__main__":
    print("ğŸš€ Python è¿è¡ŒæˆåŠŸï¼")
    interface.launch()
