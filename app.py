import cv2
import torch
import numpy as np
import gradio as gr
from PIL import Image
import open_clip
from transformers import BlipProcessor, BlipForConditionalGeneration
from sklearn.cluster import KMeans  # é¢œè‰²æå–
import ollama  # æ­Œè¯ç”Ÿæˆ

# -------------------------------
# 1ï¸âƒ£ è¯†åˆ«ç»˜ç”»å†…å®¹ (CLIP + BLIP)
# -------------------------------
model, preprocess, tokenizer = open_clip.create_model_and_transforms("ViT-B/32", pretrained="laion2b_s34b_b79k")
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# âœ… åˆå§‹åŒ– BLIPï¼ˆç”¨äºç”Ÿæˆå…·ä½“çš„ç”»é¢æè¿°ï¼‰
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device)

def ensure_pil_image(image):
    """ç¡®ä¿ `image` æ˜¯ `PIL.Image` ç±»å‹ï¼Œé˜²æ­¢ `list` ç±»å‹é”™è¯¯"""
    if isinstance(image, dict) and "composite" in image:
        image = Image.fromarray(np.array(image["composite"], dtype=np.uint8))
    elif isinstance(image, list):
        print("ğŸ“· image æ˜¯ listï¼Œè½¬æ¢ä¸º NumPy æ•°ç»„")
        image = np.array(image, dtype=np.uint8)
        image = Image.fromarray(image)
    elif isinstance(image, np.ndarray):
        image = Image.fromarray(image)
    elif not isinstance(image, Image.Image):
        raise TypeError(f"âŒ é”™è¯¯: image ç±»å‹ {type(image)} ä¸æ˜¯ PIL.Image")

    return image.convert("RGB")

def extract_visual_features(image):
    """æå–ç”»é¢é£æ ¼å…³é”®è¯ï¼ˆé¢œè‰²ã€çº¿æ¡ï¼‰"""
    image_np = np.array(image)
    gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)
    
    # **é¢œè‰²é£æ ¼**
    kmeans = KMeans(n_clusters=3, random_state=0).fit(image_np.reshape(-1, 3))
    colors = kmeans.cluster_centers_.astype(int)
    warm_ratio = sum(1 for c in colors if c[0] > 150 and c[2] < 100) / 3
    dark_ratio = sum(1 for c in colors if sum(c) < 200) / 3
    color_desc = "æ¸©æš–è€Œå……æ»¡æ´»åŠ›" if warm_ratio > 0.5 else "æ·±æ²‰è€Œç¥ç§˜" if dark_ratio > 0.5 else "è‰²å½©å’Œè°"

    # **çº¿æ¡æ„Ÿè§‰**
    edges = cv2.Canny(gray, 50, 150)
    line_desc = "çº¿æ¡æµç•…è€Œè‡ªç”±" if np.count_nonzero(edges) > 10000 else "ç®€æ´è€Œå¯Œæœ‰è¡¨ç°åŠ›"

    return f"{color_desc}ï¼Œ{line_desc}"

def describe_image_with_blip(image):
    """ä½¿ç”¨ BLIP ç”Ÿæˆç”»é¢æè¿°"""
    inputs = processor(image, return_tensors="pt").to(device)
    with torch.no_grad():
        caption = blip_model.generate(**inputs)
    return processor.decode(caption[0], skip_special_tokens=True)

def analyze_painting(image):
    """ç”Ÿæˆç”»é¢æè¿°"""

    # âœ… **å½»åº•ä¿®å¤ `image` ç±»å‹é—®é¢˜**
    image = ensure_pil_image(image)
    print(f"âœ… è½¬æ¢å image ç±»å‹: {type(image)}")

    # **è½¬æ¢ä¸º Tensor**
    image_tensor = preprocess(image).unsqueeze(0).to(device)

    # **ä½¿ç”¨ BLIP ç”Ÿæˆç”»é¢æè¿°**
    blip_description = describe_image_with_blip(image)

    # **CLIP ç”Ÿæˆæƒ…ç»ªå…³é”®è¯**
    descriptions = ["è‡ªç”±è€Œè¶…ç°å®", "æ¢¦å¹»è€Œå¥‡å¦™", "å……æ»¡æ´»åŠ›", "ç¥ç§˜è€Œæ·±é‚ƒ", "æŠ½è±¡è€Œå¯Œæœ‰å¼ åŠ›"]
    text_tokens = tokenizer(descriptions).to(device)
    
    with torch.no_grad():
        similarity = (model.encode_image(image_tensor) @ model.encode_text(text_tokens).T).softmax(dim=-1)

    clip_keyword = descriptions[similarity.argmax().item()]
    visual_keywords = extract_visual_features(image)

    return f"{blip_description}ï¼Œ{clip_keyword}ï¼Œ{visual_keywords}"

# -------------------------------
# 2ï¸âƒ£ ç”Ÿæˆæ­Œè¯ (Ollama / Gemma:2b)
# -------------------------------
def generate_lyrics(painting_description):
    """æ ¹æ®ç”»é¢æè¿°ç”Ÿæˆè¯—æ„æ­Œè¯"""
    
    prompt = f"""
    Write a poetic song inspired by this description:
    "{painting_description}"
    
    - Capture the **emotions** of the scene rather than describing it directly.
    - Use **imagery and symbolism** to create a story inspired by the painting.
    - The song should feel like a **mystical journey**, **a lonely adventure**, or **a dreamy reflection**.
    - Avoid generic words like "masterpiece" or "paintbrush". Instead, use metaphors related to art, light, and nature.
    
    Suggested format:
    
    **[Verse 1]**  
    Set the mood with visual imagery and emotional depth.  
    Introduce a **mystical character** (a lost wolf, a wandering artist, a floating soul).  
    
    **[Chorus]**  
    A repeated poetic line that captures the essence of the song.  
    
    **[Verse 2]**  
    Expand on the emotional journey, using **contrast and tension**.  
    
    Examples of poetic styles:  
    - Dreamlike and surreal (e.g., "a golden thread weaves through the sky")  
    - Mysterious and melancholic (e.g., "shadows whisper forgotten names")  
    - Soft and reflective (e.g., "memories drift like paper boats on water")  
    
    **Write in a loose poetic structure, prioritizing storytelling over rhyme.**  
    """

    response = ollama.chat(model="gemma:2b", messages=[{"role": "user", "content": prompt}])
    lyrics = response['message']['content']
    
    return format_lyrics(lyrics)

def format_lyrics(lyrics):
    """ä¼˜åŒ–æ­Œè¯æ ¼å¼ï¼Œä½¿å…¶æ›´ç¾è§‚"""
    lines = lyrics.split("\n")
    formatted_lines = [line.strip().capitalize() for line in lines if line.strip()]
    return "\n".join(formatted_lines)

# -------------------------------
# 3ï¸âƒ£ Gradio ç•Œé¢ (ç»˜ç”»è¾“å…¥)
# -------------------------------
def process_painting(image):
    """å®Œæ•´çš„ AI æ­Œè¯ç”Ÿæˆæµç¨‹"""
    painting_description = analyze_painting(image)
    print(f"ğŸ–¼ è¯†åˆ«çš„ç»˜ç”»é£æ ¼ï¼š{painting_description}")
    
    # ç”Ÿæˆæ­Œè¯
    lyrics = generate_lyrics(painting_description)
    
    return f"ğŸ¨ è¯†åˆ«çš„ç»˜ç”»é£æ ¼ï¼š{painting_description}\nğŸ¶ ç”Ÿæˆçš„æ­Œè¯ï¼š\n{lyrics}"

interface = gr.Interface(
    fn=process_painting,
    inputs=gr.Sketchpad(),  # âœ… ç›´æ¥å»æ‰ output_mode
    outputs="text",
    title="AI ç»˜ç”»æ­Œè¯ç”Ÿæˆå™¨",
    description="åœ¨ç”»å¸ƒä¸Šç»˜åˆ¶ä¸€å¹…ç”»ï¼ŒAI å°†æ ¹æ®å†…å®¹ç”Ÿæˆä¸€é¦–æ­Œè¯ ğŸµ",
)

if __name__ == "__main__":
    print("ğŸš€ Python è¿è¡ŒæˆåŠŸï¼")
    interface.launch()  # âœ… æ­£ç¡®å†™æ³•
