import os
import cv2
import dlib
import torch
import numpy as np
import torchaudio
from torchvision import models, transforms
import ollama
import gradio as gr
from TTS.api import TTS

# -------------------------------
# 1ï¸âƒ£ é¢éƒ¨æ£€æµ‹ & æƒ…ç»ªè¯†åˆ«
# -------------------------------
PREDICTOR_PATH = os.path.join(os.path.dirname(__file__), "shape_predictor_68_face_landmarks.dat")
if not os.path.exists(PREDICTOR_PATH):
    raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ° {PREDICTOR_PATH}ï¼Œè¯·ç¡®ä¿æ–‡ä»¶å·²ä¸‹è½½ï¼")

detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor(PREDICTOR_PATH)

emotion_model = models.resnet18(pretrained=True)
emotion_model.fc = torch.nn.Linear(512, 7)
emotion_model.eval()

emotion_labels = ["angry", "disgust", "fear", "happy", "neutral", "sad", "surprise"]

def detect_emotion(image_path):
    """ä½¿ç”¨ PyTorch è¿›è¡Œæƒ…ç»ªè¯†åˆ«"""
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image = cv2.resize(image, (224, 224))

    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5])
    ])
    image = transform(image).unsqueeze(0)

    with torch.no_grad():
        outputs = emotion_model(image)
        _, predicted = torch.max(outputs, 1)

    return emotion_labels[predicted.item()]

# -------------------------------
# 2ï¸âƒ£ é¢éƒ¨ç‰¹å¾æå–
# -------------------------------
def extract_facial_features(image_path):
    """ä½¿ç”¨ OpenCV å’Œ dlib æå–é¢éƒ¨ç‰¹å¾"""
    img = cv2.imread(image_path)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    faces = detector(gray)

    features = {"face_shape": "unknown", "skin_color": "unknown"}

    for face in faces:
        landmarks = predictor(gray, face)
        features["face_shape"] = "oval" if landmarks.part(0).x < landmarks.part(16).x else "round"
        features["skin_color"] = "light" if np.mean(gray) > 127 else "dark"
    
    return features

# -------------------------------
# 3ï¸âƒ£ ç”Ÿæˆæ­Œè¯ï¼ˆOllama / Gemma:2bï¼‰
# -------------------------------
def generate_lyrics(facial_features, emotion):
    """ç»“åˆé¢éƒ¨ç‰¹å¾å’Œæƒ…ç»ªç”Ÿæˆæ­Œè¯"""
    prompt = f"A poetic song about a person with {facial_features['face_shape']} face and {facial_features['skin_color']} skin, feeling {emotion}."
    
    response = ollama.chat(model="gemma:2b", messages=[{"role": "user", "content": prompt}])
    
    lyrics = response['message']['content']
    if len(lyrics.split()) < 15:
        lyrics += " This song is full of emotions and melodies that flow smoothly."
    
    return lyrics



# -------------------------------
# 4ï¸âƒ£ ç”Ÿæˆæ—‹å¾‹ï¼ˆPyTorch ç‰ˆéŸ³ä¹ç”Ÿæˆï¼‰
# -------------------------------
def generate_melody(emotion):
    """ç¡®ä¿éŸ³é¢‘è‡³å°‘ 2 ç§’"""
    sample_rate = 22050  # ç¡®ä¿é‡‡æ ·ç‡å¤Ÿé«˜
    melody_length = 2  # è‡³å°‘ 2 ç§’
    
    freqs = {
        "happy": 440,
        "sad": 220,
        "angry": 330,
        "neutral": 262,
        "surprise": 523,
    }
    
    frequency = freqs.get(emotion, 262)
    time = torch.linspace(0, melody_length, steps=int(melody_length * sample_rate))  # ä¿®æ­£ time è®¡ç®—
    melody_wave = 0.5 * torch.sin(2 * np.pi * frequency * time)

    melody_path = "melody.wav"
    torchaudio.save(melody_path, melody_wave.unsqueeze(0), sample_rate)
    
    return melody_path


# -------------------------------
# 5ï¸âƒ£ ä½¿ç”¨ FastPitch è¿›è¡Œæ­Œæ›²åˆæˆ
# -------------------------------
def synthesize_song(lyrics, melody_path):
    """ä½¿ç”¨ FastPitch è¿›è¡Œè¯­éŸ³åˆæˆ"""
    
    tts = TTS("tts_models/en/ljspeech/fast_pitch")  # âœ… æ”¹ç”¨ FastPitchï¼Œé€Ÿåº¦æ›´å¿«
    output_wav = "output.wav"
    
    # ç”Ÿæˆè¯­éŸ³å¹¶åŠ å¿«è¯­é€Ÿï¼Œé˜²æ­¢å£°éŸ³æ‹‰é•¿
    tts.tts_to_file(text=lyrics, file_path=output_wav, speed=1.1, max_decoder_steps=500)

    return output_wav



# -------------------------------
# 6ï¸âƒ£ Gradio ç•Œé¢ï¼ˆåœ¨çº¿æ’­æ”¾ï¼‰
# -------------------------------
def process_image(image):
    """å®Œæ•´çš„ AI éŸ³ä¹ç”Ÿæˆæµç¨‹"""
    cv2.imwrite("input.jpg", image)
      
    # æ£€æµ‹æƒ…ç»ª
    emotion = detect_emotion("input.jpg")
    print(f"ğŸ§ è¯†åˆ«çš„æƒ…ç»ªï¼š{emotion}")  # âœ… æ‰“å°æƒ…ç»ªè¯†åˆ«ç»“æœ

    # æå–é¢éƒ¨ç‰¹å¾
    features = extract_facial_features("input.jpg")
    
    # ç”Ÿæˆæ­Œè¯ï¼ˆç»“åˆé¢éƒ¨ç‰¹å¾ & æƒ…ç»ªï¼‰
    lyrics = generate_lyrics(features, emotion)
    
    # ç”Ÿæˆæ—‹å¾‹ï¼ˆåŸºäºæƒ…ç»ªï¼‰
    melody = generate_melody(emotion)
    
    # åˆæˆæ­Œæ›²
    song = synthesize_song(lyrics, melody)
    
    return lyrics, melody, song


interface = gr.Interface(
    fn=process_image,
    inputs=gr.Image(type="numpy"),
    outputs=[
        "text",  # æ­Œè¯æ–‡æœ¬
        gr.Audio(type="filepath", format="wav"),  # ğŸµ æ—‹å¾‹ï¼ˆåœ¨çº¿æ’­æ”¾ï¼‰
        gr.Audio(type="filepath", format="wav")   # ğŸ¤ ç”Ÿæˆçš„æ­Œæ›²ï¼ˆåœ¨çº¿æ’­æ”¾ï¼‰
    ],
    title="AI æ­Œæ›²ç”Ÿæˆå™¨",
    description="ä¸Šä¼ ä¸€å¼ ç…§ç‰‡ï¼ŒAI å°†æ ¹æ®ä½ çš„é¢éƒ¨ç‰¹å¾ç”Ÿæˆä¸€é¦–æ­Œæ›² ğŸµ"
)

if __name__ == "__main__":
    print("ğŸš€ Python è¿è¡ŒæˆåŠŸï¼")
    interface.launch()
